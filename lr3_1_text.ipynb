{"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"id":"4K0ZgdqlVyRY"},"source":["# Text Classification\n","\n","In this problem, you will be again analyzing the Twitter data we extracted from 2016 using [this](https://dev.twitter.com/overview/api) api. This time, we extracted the tweets posted by the following six Twitter accounts: `realDonaldTrump, mike_pence, GOP, HillaryClinton, timkaine, TheDemocrats`.\n","\n","For every tweet, we collected two pieces of information:\n","- `screen_name`: the Twitter handle of the user tweeting and\n","- `text`: the content of the tweet.\n","\n","We divided the tweets into two parts - the train and test sets.  The training set contains both the `screen_name` and `text` of each tweet; the test set only contains the `text`.\n","\n","The overarching goal of the problem is to infer the political inclination (whether **R**epublican or **D**emocratic) of the author from the tweet text. The ground truth (i.e., true class labels) are determined from the `screen_name` of the tweet as follows:\n","- **R**: `realDonaldTrump, mike_pence, GOP`\n","- **D**: `HillaryClinton, timkaine, TheDemocrats`\n","\n","We can treat this as a binary classification problem. We'll follow this common structure to tackling this problem:\n","\n","1. **preprocessing**: clean up the raw tweet text using regular expressions, and produce class labels\n","2. **features**: construct bag-of-words feature vectors\n","3. **classification**: learn a binary classification model using [`scikit-learn`](http://scikit-learn.org/stable/modules/classes.html).\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"utCwhv4TVyRa","executionInfo":{"status":"ok","timestamp":1703266588626,"user_tz":-120,"elapsed":1706,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}}},"outputs":[],"source":["import collections\n","import string\n","import numpy as np\n","import sklearn\n","import sklearn.feature_extraction\n","import sklearn.svm\n","import sklearn.metrics\n","import gzip\n","import re\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"uW4q7e55VyRb"},"source":["## A. Text Processing\n","\n","### Q1 Preprocessing\n","Your first task is to fill in the following function which processes and tokenizes raw text. You will need to preprocess the tokens by applying the operators _in the following order_.\n","\n","1. Convert the text to lower case.\n","2. Remove any URLs, which in this case will all be of the form `http://t.co/<alphanumeric characters>`.\n","3. Remove all trailing `'s` characters, followed by other apostrophes:\n","   - remove trailing `'s`: `Children's` becomes `children`\n","   - omit other apostrophes: `don't` becomes `dont`\n","4. Remove all non-alphanumeric (i.e., A-Z, a-z, 0-9) characters (replacing them with a single space)\n","5. Split the remaining text by whitespace into an array of individual words\n","6. Discard empty strings (i.e., if the string after processing above is equal to \"\"), return an empty array `[]` rather than `['']`"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Fc13YS0CVyRc","executionInfo":{"status":"ok","timestamp":1703266645824,"user_tz":-120,"elapsed":301,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}}},"outputs":[],"source":["def preprocess(text):\n","    \"\"\" Normalizes case and handles punctuation\n","\n","    args:\n","        text: str -- raw text\n","\n","    Outputs:\n","        list(str): tokenized text\n","    \"\"\"\n","     # Convert text to lowercase\n","    text = text.lower()\n","\n","    # Remove URLs\n","    text = re.sub(r'http://t.co/\\w+', '', text)\n","\n","    # Remove trailing 's and other apostrophes\n","    text = re.sub(r\"'s\\b|'\", '', text)\n","\n","    # Remove non-alphanumeric characters\n","    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n","\n","    # Split the text by whitespace into individual words\n","    words = text.split()\n","\n","    # Discard empty strings\n","    words = [word for word in words if word != '']\n","\n","    return words"]},{"cell_type":"markdown","metadata":{"id":"rDhP3WOEVyRd"},"source":["### Q2 Loading Data\n","\n","Using this preprocess function, load the data from the relevant csv files and return a list of the parsed tweets, plus a flag indicating whether or not the tweet is from a republican (i.e., one of the three usernames mentioned above); for the test data, where no screen name is given, provide `None` as the flag).  Note that this function should take less than a second if you've implemented the above preprocessing function efficiently."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"LNSqIqo3VyRd","executionInfo":{"status":"ok","timestamp":1703266964548,"user_tz":-120,"elapsed":4,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}}},"outputs":[],"source":["# @mugrade.local_tests\n","def read_data():\n","    \"\"\"Reads the dataset from the tweets_train.csv.gz and tweets_test.csv.gz files\n","\n","    return : Tuple (data_train, data_test)\n","        data_train : List[Tuple[is_republican, tokenized_tweet]]\n","            is_republican : bool -- True if tweet is from a republican\n","            tokenized_tweet : List[str] -- the tweet, tokenized by preprocess()\n","        data_test : List[Tuple[None, tokenized_tweet]\n","            None: the Python constant \"None\"\n","            tokenized_tweet : List[str] -- the tweet, tokenized by preprocess()\n","    \"\"\"\n","    train_data = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/lab3/tweets_train.csv')  # Load train data\n","    test_data = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/lab3/tweets_test.csv')    # Load test data\n","\n","    # Apply preprocessing function to train and test data\n","    data_train = [(True if row['screen_name'] in ['realDonaldTrump', 'mike_pence', 'GOP'] else False, preprocess(row['text'])) for _, row in train_data.iterrows()]\n","    data_test = [(None, preprocess(row['text'])) for _, row in test_data.iterrows()]\n","\n","    return data_train, data_test"]},{"cell_type":"markdown","metadata":{"id":"VIsFz7WVVyRe"},"source":["## B. Feature Construction\n","\n","The next step is to derive feature vectors from the tokenized tweets. In this section, you will be constructing a bag-of-words [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) feature vector.\n","\n","\n","### Q3 Word distributions\n","The number of possible words is prohibitively large, and not all words are useful for our task. We will begin by filtering the vectors using a common heuristic: We calculate a frequency distribution of words in the corpus and remove words at the head (most frequent) and tail (least frequent) of the distribution. Most frequently used words (often called stopwords) provide very little information about the similarity of two pieces of text. Words with extremely low frequency tend to be typos.\n","\n","We will now implement a function that counts the number of times that each token is used in the training corpus. You should return a [`collections.Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) object with the number of times that each word appears in the dataset."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PeOd1N2nPwx","executionInfo":{"status":"ok","timestamp":1703266970081,"user_tz":-120,"elapsed":2013,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}},"outputId":"65323d4e-f25c-4f47-f2c0-26f2c1ef211e"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":17,"metadata":{"id":"hY2_r_h4VyRe","executionInfo":{"status":"ok","timestamp":1703266971302,"user_tz":-120,"elapsed":3,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}}},"outputs":[],"source":["def get_distribution(data):\n","    \"\"\" Calculates the word count distribution.\n","\n","    args:\n","        data -- the training or testing data\n","\n","    return : collections.Counter -- the distribution of word counts\n","    \"\"\"\n","    word_count = collections.Counter()\n","    for _, tokenized_tweet in data:\n","        word_count.update(tokenized_tweet)\n","    return word_count"]},{"cell_type":"markdown","metadata":{"id":"eW9bsvDZVyRf"},"source":["We can use this function, once implemented properly, to get a sense of the distribution of words in the training set."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"TS253KLdVyRg","outputId":"e753251f-7799-41b3-d857-e4a26baef5a3","colab":{"base_uri":"https://localhost:8080/","height":434},"executionInfo":{"status":"ok","timestamp":1703266977430,"user_tz":-120,"elapsed":3735,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["data_train, _ = read_data()  # Load the training data\n","dist_train = get_distribution(data_train)  # Get word count distribution\n","\n","plt.yscale(\"log\")\n","plt.xscale(\"log\")\n","plt.hist(dist_train.values(), bins=np.logspace(0, 4, 100))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6HFktDpgVyRg"},"source":["Notice that the plot looks roughly linear on this log-log plot (for those we are curious, this is a phenomenon known as [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law)).  For us, however, it largely means that there are many words that appear many times (i.e., common words like \"the\"), which thus won't be very predictive for our task, because they are unlike to differentiate Republican vs. Democratic tweets.  There are also words that appear very infrequently, which also means that they aren't going to be very predictive, but for a different reason: these words likely won't occur very often in the test set, and thus will largely just cause the classifier to overfit to the training set.  However, instead of removing these words manually, in the next question, we will use the TFIDF weighting and vectorizer to both remove overly-common words and exclude too-infrequent words."]},{"cell_type":"markdown","metadata":{"id":"lfakhNfWVyRh"},"source":["### Q4 Vectorizing\n","\n","Now we have each tweet as a list of words, excluding words with high and low frequencies. We want to convert these into a sparse feature matrix, where each row corresponds to a tweet and each column to a possible word. We can use `scikit-learn`'s [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to do this quite easily.\n","\n","Instructions:\n"," - By default, the `TfidfVectorizer` does its own tokenization, but we've already done it above, so you need to pass `preprocessor = lambda x : x, tokenization = lambda x : x, token_pattern=None` as arguments to the class constructor.\n"," - The vectorizer can filter words that are too uncommon or too common: to do this, set the `min_df=5` argument (words must be contained in more than 5 tweets), and `max_df=0.4` argument (filter out words contained in more than 40% of tweets)\n"," - You should use only the training data to `fit` or `fit_transform` the vectorizer."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"0uf2vVuFVyRh","executionInfo":{"status":"ok","timestamp":1703267190642,"user_tz":-120,"elapsed":6,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}}},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import numpy as np\n","from scipy.sparse import csr_matrix\n","\n","def create_features(train_data, test_data):\n","    \"\"\"creates the feature matrices and label vector for the training and test sets.\n","\n","    args:\n","        train_data, test_data : output of read_data() function\n","\n","    returns: Tuple[train_features, train_labels, test_features]\n","        train_features : scipy.sparse.csr.csr_matrix -- TFIDF feature matrix for the training set\n","        train_labels : np.array[num_train] -- a numpy vector, where 1 stands for Republican and 0 stands for Democrat\n","        test_features : scipy.sparse.csr.csr_matrix -- TFIDF feature matrix for the test set\n","    \"\"\"\n","    train_tweets = [tweet for _, tweet in train_data]\n","    train_labels = np.array([1 if is_republican else 0 for is_republican, _ in train_data])\n","\n","    # Extracting tokenized tweets for test data\n","    test_tweets = [tweet for _, tweet in test_data]\n","\n","    # Initialize TfidfVectorizer\n","    vectorizer = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, token_pattern=None,\n","                                 min_df=5, max_df=0.4)\n","\n","    # Fit TfidfVectorizer on training data and transform both train and test data\n","    train_features = vectorizer.fit_transform(train_tweets)\n","    test_features = vectorizer.transform(test_tweets)\n","\n","    return train_features, train_labels, test_features"]},{"cell_type":"code","source":["data_train, data_test = read_data()\n","train_features, train_labels, test_features = create_features(data_train, data_test)\n","print(\"Train Features shape:\", train_features.shape)\n","print(\"Test Features shape:\", test_features.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GPihvfZ5pajr","executionInfo":{"status":"ok","timestamp":1703267388406,"user_tz":-120,"elapsed":6529,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}},"outputId":"3aab135c-4081-4a8d-b099-3bd458f17221"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Features shape: (17298, 4769)\n","Test Features shape: (1000, 4769)\n"]}]},{"cell_type":"markdown","metadata":{"id":"zOLRyBecVyRi"},"source":["Observe that the created matrices are very sparse, which is to be expected especially for tweets (given that each tweet can only contain relatively few words)."]},{"cell_type":"markdown","metadata":{"id":"YxnSx3JQVyRi"},"source":["## C. Classification\n","\n","We are now ready to put it all together and train the classification model.\n","\n","You will be using the Support Vector Machine [`sklearn.svm.LinearSVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn). This class implements a linear SVM as we described in class, though of course, the details vary a little bit with this particular implementation.\n","\n","### Q5 Training a classifier\n","\n","Let's begin by training a classifier. You should specifically train a `LinearSVC` with a given set of features and labels, plus the regularization parameter specified by `C`.  You can additionally include as arguments to the `LinearSVC` class the `loss = \"hinge\"` argument (so that this is a typical SVM), and the `random_state=0` argument (to avoid any randomness in the training).  **Additionally, you should use the `max_iter=10000` argument to make sure that you run for enough iterations to avoid any failure to converge given the regularization parameters we use**."]},{"cell_type":"code","execution_count":25,"metadata":{"id":"nx-EVknwVyRi","executionInfo":{"status":"ok","timestamp":1703267471351,"user_tz":-120,"elapsed":305,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}}},"outputs":[],"source":["from sklearn.svm import LinearSVC\n","def train_classifier(features, labels, C):\n","    \"\"\"learns a classifier from the input features and labels using a specified kernel function\n","\n","    args:\n","        features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n","        labels : numpy.ndarray(bool): binary vector of class labels\n","        C : float -- C regularization parameters\n","\n","    returns: sklearn.svm.LinearSVC -- classifier trained on data\n","    \"\"\"\n","    classifier = LinearSVC(loss=\"hinge\", C=C, random_state=0, max_iter=10000)\n","    classifier.fit(features, labels)\n","    return classifier"]},{"cell_type":"code","source":["data_train, _ = read_data()\n","train_features, train_labels, _ = create_features(data_train, data_test)\n","classifier = train_classifier(train_features, train_labels, C=1.0)\n","\n","print(classifier)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y99sTiv7p3_n","executionInfo":{"status":"ok","timestamp":1703267804147,"user_tz":-120,"elapsed":6425,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}},"outputId":"7c6df66a-ebd0-4ca0-ccb6-187183b6ebe9"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["LinearSVC(loss='hinge', max_iter=10000, random_state=0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"rYegTyJuVyRi"},"source":["### Q6 Cross validation\n","\n","After building the function to train this classifier, let's now use a validation set to pick the optimal value of `C`, out of the choices of `(0.01, 0.1, 1.0, 10.0)`.  The basic approach here will be to split the training set into the first 10000 samples for the training set, and the remainder for the validation set, allowing you to choose the best parameter to use on the training set.  To evaluate the quality of the classifier, you will use the [F1 score](https://en.wikipedia.org/wiki/F-score), a common metric for text classification, which you can compute using the `sklearn.metrics.f1_score` function.\n","\n","Specifically, you should implement the function below, which will compute the training and validation F1 score for different classifiers trained with different values of C."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"ZxIkajtXVyRj","executionInfo":{"status":"ok","timestamp":1703267818106,"user_tz":-120,"elapsed":307,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}}},"outputs":[],"source":["from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split\n","def evaluate_classifier(features, labels, C = (0.01, 0.1, 1.0, 10., 100.), train_length=10000):\n","    \"\"\" Train multiple classifier based on the first train_length features of features/labels,\n","        one for each regularization parameter supplied in C, and return train/validation f1\n","        scores for each of the classifiers\n","\n","    args:\n","        features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n","        labels : numpy.ndarray(bool): binary vector of class labels\n","        C : Tuple[float] -- tuple of C regularization parameters\n","        train_length: int -- use _first_ train_length features for training (and the rest of validation)\n","\n","    return : List[Tuple[float, float]] -- list of F1 scores for training/validation for each C parameter\n","    \"\"\"\n","    f1_scores = []\n","\n","    # Split the data into training and validation sets\n","    train_features, val_features, train_labels, val_labels = train_test_split(\n","        features, labels, train_size=train_length, random_state=0, stratify=labels\n","    )\n","\n","    for c in C:\n","        # Train a classifier for each value of C\n","        classifier = train_classifier(train_features, train_labels, C=c)\n","\n","        # Predict on the training set and calculate F1 score\n","        train_pred = classifier.predict(train_features)\n","        train_f1 = f1_score(train_labels, train_pred)\n","\n","        # Predict on the validation set and calculate F1 score\n","        val_pred = classifier.predict(val_features)\n","        val_f1 = f1_score(val_labels, val_pred)\n","\n","        f1_scores.append((train_f1, val_f1))\n","\n","    return f1_scores"]},{"cell_type":"code","source":["data_train, _ = read_data()\n","train_features, train_labels, _ = create_features(data_train, data_test)\n","if train_features.shape[0] == 0:\n","    raise ValueError(\"No training features generated.\")\n","\n","f1_scores = evaluate_classifier(train_features, train_labels)\n","print(f\"F1 Scores for different values of C: {f1_scores}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5yviDtF5rQ6Q","executionInfo":{"status":"ok","timestamp":1703267917645,"user_tz":-120,"elapsed":13906,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}},"outputId":"5279799c-cbee-48cc-8802-f234a8455ceb"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 Scores for different values of C: [(0.7568660087980026, 0.7431658209199541), (0.879591184924944, 0.8438283439020794), (0.9504769636695758, 0.8855524079320114), (0.9843561973525873, 0.877426511369939), (0.9954914337240757, 0.8508333333333333)]\n"]}]},{"cell_type":"markdown","metadata":{"id":"hARWczSdVyRj"},"source":["### Q7 Classifying new Tweets\n","\n","Finally, let's put this all together.  Using the _best_ `C` value you found in the previous part (i.e., build the classifiers and test which `C` value out of `(0.01, 0.1, 1.0, 10., 100.)` gives the highest F1 score on the _validation_ set (you can hardcode this value into the function below), train a classifier on the _entire_ training set, and make predictions for the test set.  You won't be able to evaluate how accurate these predictions are, of course, but you can use this classifier to classify tweets as being from Republican or Democratic sources (or perhaps more precisely, from being from one of the three aforementioned Republicans or three Democrats during the 2016 election)."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"QEyjZ1fXVyRj","executionInfo":{"status":"ok","timestamp":1703267317297,"user_tz":-120,"elapsed":253,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}}},"outputs":[],"source":["def predict_test(train_features, train_labels, test_features):\n","    \"\"\"train the classifier on the training set and return predictions on the test set\n","\n","    args:\n","        train_features: scipy.sparse.csr.csr_matrix -- sparse matrix of training features\n","        train_labels : numpy.ndarray(bool): binary vector of training class labels\n","        test_features: scipy.sparse.csr.csr_matrix -- sparse matrix of test set features\n","\n","    return : numpy.ndarray(bool): array of predictions on the test set\n","    \"\"\"\n","    best_C = 1.0  # Replace this value with the best C obtained from validation\n","\n","    # Train the classifier on the entire training set with the best C value\n","    classifier = train_classifier(train_features, train_labels, C=best_C)\n","\n","    # Make predictions on the test set\n","    test_predictions = classifier.predict(test_features)\n","\n","    return test_predictions"]},{"cell_type":"code","source":["data_train, data_test = read_data()\n","train_features, train_labels, test_features = create_features(data_train, data_test)\n","if train_features.shape[0] == 0:\n","    raise ValueError(\"No training features generated.\")\n","\n","# Use the predict_test function to get predictions on the test set\n","test_predictions = predict_test(train_features, train_labels, test_features)\n","print(\"Predictions on the test set:\", test_predictions)"],"metadata":{"id":"ZwQE5-QRrnRS","executionInfo":{"status":"ok","timestamp":1703267963942,"user_tz":-120,"elapsed":6014,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}},"outputId":"0fc2367f-4b9e-42b2-aa0d-0dea79802589","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions on the test set: [0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1\n"," 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 0\n"," 0 0 0 0 0 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0\n"," 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1\n"," 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1\n"," 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1\n"," 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0\n"," 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0\n"," 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1\n"," 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0\n"," 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0\n"," 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 1 0\n"," 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0\n"," 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0\n"," 0 1 0 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0\n"," 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 0\n"," 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0\n"," 0 0 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1\n"," 1 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1\n"," 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0\n"," 0 0 1 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0\n"," 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1\n"," 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1\n"," 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n"," 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0\n"," 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0\n"," 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 0\n"," 0]\n"]}]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}