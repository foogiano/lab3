{"cells":[{"cell_type":"markdown","metadata":{"id":"wT3HAVQ4ZMGI"},"source":["# Natural Language Processing\n","\n","In this problem you will develop two techniques for analyzing free text documents: a bag of words approach based on a TFIDF matrix and an n-gram language model.\n","\n","You'll be applying your models to the text from the Federalist Papers.  The Federalist papers were a series of essays written in 1787 and 1788 by Alexander Hamilton, James Madison, and John Jay (they were published anonymously at the time), that promoted the ratification of the U.S. Constitution. If you're curious, you can read more about them here: https://en.wikipedia.org/wiki/The_Federalist_Papers. They are a particularly interesting data set because although the authorship of most of the essays has been long known since around the deaths of Hamilton and Madison, there was still some question about the authorship of certain articles into the 20th century.  You'll use document vectors and language models to do this analysis for yourself."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"iZhv4BL2ZMGK","executionInfo":{"status":"ok","timestamp":1703267996133,"user_tz":-120,"elapsed":6,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}}},"outputs":[],"source":["import collections # we found collections.Counter and collections.defaultdict useful\n","import itertools\n","import gzip\n","import re\n","import numpy as np\n","import scipy.sparse as sp"]},{"cell_type":"markdown","metadata":{"id":"py89eeL2ZMGL"},"source":["## The dataset\n","\n","You'll use a copy of the Federalist Papers downloaded from Project Guttenberg, available here: http://www.gutenberg.org/ebooks/18.  Specifically, the \"pg18.txt.gz\" file included with the homework is the raw text downloaded from Project Guttenberg.  To ensure that everyone starts with the exact same corpus, we are providing you the code to load and tokenize this document, as given below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aaH-jCsnZMGM"},"outputs":[],"source":["def load_federalist_corpus(filename=\"pg18.txt.gz\", encoding='utf8'):\n","    \"\"\" Load the federalist papers as a tokenized list of strings\"\"\"\n","    with gzip.open(filename, \"rt\", encoding=encoding) as f:\n","        data = f.read()\n","    papers = data.split(\"FEDERALIST\")\n","\n","    # all start with \"To the people of the State of New York:\" (sometimes . instead of :)\n","    # all end with PUBLIUS (or no end at all)\n","    locations = [(i,[-1] + [m.end()+1 for m in re.finditer(r\"of the State of New York\", p)],\n","                 [-1] + [m.start() for m in re.finditer(r\"PUBLIUS\", p)]) for i,p in enumerate(papers)]\n","    papers_content = [papers[i][max(loc[1]):max(loc[2])] for i,loc in enumerate(locations)]\n","\n","    # discard entries that are not actually a paper\n","    papers_content = [p for p in papers_content if len(p) > 0]\n","\n","    # replace all whitespace with a single space\n","    papers_content = [re.sub(r\"\\s+\", \" \", p).lower() for p in papers_content]\n","\n","    # add spaces before all punctuation, so they are separate tokens\n","    punctuation = set(re.findall(r\"[^\\w\\s]+\", \" \".join(papers_content))) - {\"-\",\"'\"}\n","    for c in sorted(punctuation, reverse=True):\n","        papers_content = [p.replace(c, \" \"+c+\" \") for p in papers_content]\n","    papers_content = [re.sub(r\"\\s+\", \" \", p).lower().strip() for p in papers_content]\n","\n","    authors = [tuple(re.findall(\"MADISON|JAY|HAMILTON\", a)) for a in papers]\n","    authors = [a for a in authors if len(a) > 0]\n","\n","    numbers = [re.search(r\"No\\. \\d+\", p).group(0) for p in papers if re.search(r\"No\\. \\d+\", p)]\n","\n","    return papers_content, authors, numbers\n"]},{"cell_type":"markdown","metadata":{"id":"pMABRdxUZMGN"},"source":["You're welcome to dig through the code here if you're curious, but it's more important that you look at the objects that the function returns.  The `papers` object is a list of strings, each one containing the full content of one of the Federalist Papers.  All tokens (words) in the text are separated by a single space (this includes some punctuation tokens, which have been modified to include spaces both before and after the punctuation. The `authors` object is a list of lists, which each list contains the author (or potential authors) of a given paper.  Finally, the `numbers` list just contains the number of each Federalist paper.  You won't need to use this last one, but you may be curious to compare the results of your textual analysis to the opinion of historians."]},{"cell_type":"markdown","metadata":{"id":"c-t35QTDZMGO"},"source":["### Q1: Bag of words, and TFIDF\n","\n","In this portion of the question, you'll use a bag of words model to describe the corpus, and write routines to build a TFIDF matrix and a cosine similarity function.  Specifically, you should first implement the TFIDF function below.  **Note that you need to do this manually, and not via the scikit-learn Tfidf vectorizers you used in a previous assignment question**  This should return a _sparse_ TFIDF matrix (as for the Graph Library assignment, make sure to directly create a sparse matrix using `scipy.sparse.coo_matrix()` rather than create a dense matrix and then convert it to be sparse).\n","\n","You should create the tfidf vector using numpy matrix operations and not use an existing implementation.\n","\n","Important: make sure you do _not_ include the empty token `\"\"` as one of your terms."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"aQe8TUBLZMGP","executionInfo":{"status":"ok","timestamp":1703268218112,"user_tz":-120,"elapsed":226,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}}},"outputs":[],"source":["import numpy as np\n","from scipy.sparse import coo_matrix\n","def tfidf(docs):\n","    \"\"\"\n","    Create TFIDF matrix.  This function creates a TFIDF matrix from the\n","    docs input.\n","\n","    Args:\n","        docs: list of strings, where each string represents a space-separated\n","              document\n","\n","    Returns: tuple: (tfidf_matrix, all_words)\n","        tfidf_matrix: sparse matrix (in any scipy sparse format) of size (# docs) x\n","               (# total unique words), where i,j entry is TFIDF score for\n","               document i and term j\n","        all_words: list of strings, where the ith element indicates the word\n","                   that corresponds to the ith column in the TFIDF matrix\n","    \"\"\"\n","    all_words = sorted(set(word for doc in docs for word in doc.split() if word != \"\"))\n","\n","    # Create a dictionary to map words to their indices\n","    word_to_index = {word: i for i, word in enumerate(all_words)}\n","\n","    # Count term frequencies for each term in each document\n","    doc_word_count = np.zeros((len(docs), len(all_words)))\n","    for i, doc in enumerate(docs):\n","        words = doc.split()\n","        word_counts = dict.fromkeys(all_words, 0)\n","        for word in words:\n","            if word != \"\":\n","                word_counts[word] += 1\n","        for word, count in word_counts.items():\n","            doc_word_count[i, word_to_index[word]] = count\n","\n","    # Calculate Term Frequency (TF)\n","    tf = doc_word_count / np.sum(doc_word_count, axis=1, keepdims=True)\n","\n","    # Calculate Document Frequency (DF)\n","    df = np.count_nonzero(doc_word_count, axis=0)\n","\n","    # Calculate Inverse Document Frequency (IDF)\n","    idf = np.log(len(docs) / df)\n","\n","    # Compute TFIDF scores\n","    tfidf_matrix = tf * idf\n","\n","    # Create a sparse TFIDF matrix\n","    tfidf_sparse = coo_matrix(tfidf_matrix)\n","\n","    return tfidf_sparse, all_words"]},{"cell_type":"markdown","metadata":{"id":"DZYstdTmZMGP"},"source":["Our version results the following result (just showing the type, size, and # of non-zero elements):\n","\n","    <86x8686 sparse matrix of type '<type 'numpy.float64'>'\n","        with 57607 stored elements in Compressed Sparse Row format>"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IsFij28Q-6rd","executionInfo":{"status":"ok","timestamp":1703273035759,"user_tz":-120,"elapsed":17394,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}},"outputId":"2e8a381c-4c98-43c9-fc85-c1f04b826e94"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["tfidf_matrix, all_words = tfidf(\"/content/drive/MyDrive/Colab_Notebooks/lab3/pg18.txt.gz\")\n","\n","# Output the characteristics of the TFIDF matrix\n","print(tfidf_matrix)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-qHy1saas0MP","executionInfo":{"status":"ok","timestamp":1703273087058,"user_tz":-120,"elapsed":352,"user":{"displayName":"Іон Аркадійович Костинян","userId":"16695049964818840622"}},"outputId":"9b86db09-01d5-49b9-dd07-c2a00cf50e96"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["  (0, 1)\t2.2155737160044158\n","  (1, 12)\t4.007333185232471\n","  (2, 20)\t2.3978952727983707\n","  (3, 19)\t3.3141860046725258\n","  (4, 24)\t2.3978952727983707\n","  (5, 14)\t2.6210388241125804\n","  (6, 19)\t3.3141860046725258\n","  (7, 24)\t2.3978952727983707\n","  (8, 1)\t2.2155737160044158\n","  (9, 13)\t4.007333185232471\n","  (10, 22)\t3.3141860046725258\n","  (11, 16)\t3.3141860046725258\n","  (12, 25)\t3.3141860046725258\n","  (13, 14)\t2.6210388241125804\n","  (14, 1)\t2.2155737160044158\n","  (15, 7)\t4.007333185232471\n","  (16, 27)\t4.007333185232471\n","  (17, 6)\t4.007333185232471\n","  (18, 22)\t3.3141860046725258\n","  (19, 16)\t3.3141860046725258\n","  (20, 25)\t3.3141860046725258\n","  (21, 14)\t2.6210388241125804\n","  (22, 1)\t2.2155737160044158\n","  (23, 5)\t4.007333185232471\n","  (24, 20)\t2.3978952727983707\n","  :\t:\n","  (30, 20)\t2.3978952727983707\n","  (31, 24)\t2.3978952727983707\n","  (32, 14)\t2.6210388241125804\n","  (33, 11)\t2.908720896564361\n","  (34, 20)\t2.3978952727983707\n","  (35, 20)\t2.3978952727983707\n","  (36, 17)\t4.007333185232471\n","  (37, 23)\t4.007333185232471\n","  (38, 1)\t2.2155737160044158\n","  (39, 18)\t3.3141860046725258\n","  (40, 10)\t3.3141860046725258\n","  (41, 11)\t2.908720896564361\n","  (42, 3)\t4.007333185232471\n","  (43, 1)\t2.2155737160044158\n","  (44, 21)\t4.007333185232471\n","  (45, 15)\t3.3141860046725258\n","  (46, 2)\t4.007333185232471\n","  (47, 4)\t4.007333185232471\n","  (48, 0)\t3.3141860046725258\n","  (49, 24)\t2.3978952727983707\n","  (50, 26)\t4.007333185232471\n","  (51, 24)\t2.3978952727983707\n","  (52, 0)\t3.3141860046725258\n","  (53, 15)\t3.3141860046725258\n","  (54, 28)\t4.007333185232471\n"]}]},{"cell_type":"markdown","metadata":{"id":"ha9_wiuwZMGQ"},"source":["### Q2: Cosine Similarity\n","\n","Next, implement the following simple function that takes the X_tfidf matrix (though it could also take simple term frequency matrices, etc), and compute a matrix of all pair-wise cosine similarities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPZQ8uKSZMGQ"},"outputs":[],"source":["# @mugrade.local_tests\n","def cosine_similarity(X):\n","    \"\"\"\n","    Return a matrix of cosine similarities.\n","\n","    Args:\n","        X: sparse matrix of TFIDF scores or term frequencies\n","\n","    Returns:\n","        M: dense numpy array of all pairwise cosine similarities.  That is, the\n","           entry M[i,j], should correspond to the cosine similarity between the\n","           ith and jth rows of X.\n","    \"\"\"\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"q9qav09_ZMGR"},"source":["### Q3 Analyzing document authorship\n","\n","Finally, use this model to analyze potential authorship of the unknown Federalist Papers.  Specifically, compute the average cosine similarity between all the _known_ Hamilton papers and all the _unknown_ papers (and similarly between known Madison and unknown, and Jay and unknown).  Fill out the following function to compute and return these averages.\n","\n","Hints:\n","\n","1. fit a single TFIDF encoding to all papers and transform all papers using it before computing the similarity measure\n","2. for the cosine similarity to be useful when comparing documents, they must all be encoded the same way. Transform all papers together before comparing cosine similarity.\n","3. the unknown papers have author=`(\"HAMILTON\",\"MADISON\")`;"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9PHb6jioZMGR"},"outputs":[],"source":["def author_cosine_similarity(docs, authors):\n","    \"\"\"\n","    Return a tuple of average cosine similarities between all the known papers for a given author\n","    and all the unknown papers.\n","\n","    Args:\n","        docs: list of strings, where each string represents a space-separated\n","              document\n","        authors: list of lists, which each list contains the author (or potential authors) of a given document\n","\n","    Returns: tuple: (hamilton_mcs, madison_mcs, jay_mcs)\n","        hamilton_mcs: Average cosine similarity between all the known Hamilton papers and all the unknown papers.\n","        madison_mcs: Average cosine similarity between all the known Madison papers and all the unknown papers.\n","        jay_mcs: Average cosine similarity between all the known Jay papers and all the unknown papers.\n","    \"\"\"\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"BJN4Z_SPZMGR"},"source":["Run the local test case, `author_cosine_similarity(*load_federalist_corpus()[:2])`, to see which author has the highest degree of similarity with the unknown essays."]},{"cell_type":"markdown","metadata":{"id":"zXXt3mMEZMGS"},"source":["\n","## N-gram language model\n","\n","### Q4 Building an n-gram language model\n","\n","In this question, you will implement an n-gram model to be able to model the language used in the Federalist Papers in a more structured manner than the simple bag of words approach.  You will fill in the following class, and should do it in two parts:\n","\n","### Part A: Initializing the language model\n","\n","First, implement the `__init__()` function in the `LanguageModel` class.  You should do this by building a two-level dictionary (in fact, we used the `collections.defaultdict` class, but this only make a few things a little bit shorter ... you are free to use it or not) `self.counts`, where the first key refers to the previous $n-1$ tokens, and the second key refers to the $n$-th token, and the value simply stores the count of the number of times this combination was seen.  For ease of use in later functions, we also created a `self.count_sums`, which contained the number of total times each $n-1$ combination was seen. We also build a `self.vocabulary` variable, which is just a `set` object containing all the unique words across the entire set of the input document.\n","\n","### Part B: Computing perplexity\n","\n","Next, implement the `perplexity()` function, which takes a text sample and computes the perplexity of this sample under the model.  We'll make a small tweak to the formula for perplexity in the notes because we are skipping the first $n-1$ tokens; since we only compute (#words - $(n-1)$) terms in the sum, you should divide by this denominator and not just #words). Be careful to not multiply together probabilities that get too small (hint: instead of taking the log of a large product, take the sum of the logs).\n","\n","Specifically,\n","\n","\\begin{equation}\n","\\mbox{Perplexity} = 2^{\\frac{-\\log_2 P\\left(\\mathrm{word}_1, \\ldots, \\mathrm{word}_N\\right)}{N-(n+1)}} = 2^{\\text{^}}\\left(\\frac{-\\log_2 P(\\mathrm{word}_1, \\ldots, \\mathrm{word}_N)}{N-(n-1)}\\right)\n","\\end{equation}\n","\n","where\n","\n","\\begin{equation}\n","\\log_2 P(\\mathrm{word}_1, \\ldots, \\mathrm{word}_N) = \\sum_{i=n}^N \\log_2 P(\\mathrm{word}_i | \\mathrm{word}_{i-n+1}, \\ldots, \\mathrm{word}_{i-1})\n","\\end{equation}\n","\n","You'll want to be careful about vocabulary sizes when it comes to the Laplace smoothing term: make sure your vocabulary size $D$ is equal to the total number of unique words that occur in either the original data used to build the language model _or_ in the text we are evaluating the perplexity of (so the size of the union of the two)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aWgDQCjwZMGS"},"outputs":[],"source":["class LanguageModel:\n","    def __init__(self, docs, n):\n","        \"\"\"\n","        Initialize an n-gram language model.\n","\n","        Args:\n","            docs: list of strings, where each string represents a space-separated\n","                  document\n","            n: integer, degree of n-gram model\n","        \"\"\"\n","\n","        self.counts = collections.defaultdict(dict) # Dict from space-separated \"previous words\" to a Dict of (next word, count).\n","        self.count_sums = {} # Dict from space-separated \"previous words\" to the total number of times they appear\n","\n","        pass\n","\n","    def perplexity(self, text, alpha=1e-3):\n","        \"\"\"\n","        Evaluate perplexity of model on some text.\n","\n","        Args:\n","            text: string containing space-separated words, on which to compute\n","            alpha: constant to use in Laplace smoothing\n","\n","        Note: for the purposes of smoothing, the vocabulary size (i.e, the D term)\n","        should be equal to the total number of unique words used to build the model\n","        _and_ in the input text to this function.\n","\n","        Returns: perplexity\n","            perplexity: floating point value, perplexity of the text as evaluated\n","                        under the model.\n","        \"\"\"\n","        pass"]},{"cell_type":"markdown","metadata":{"id":"zUafY9zJZMGT"},"source":["### Q5 Author identification via language models\n","\n","Using this model, evaluate the mean of the perplexity of the unknown Federalist papers for the language models from each of the three authors (again, using `n=3` and the default of `alpha=1e-3`). Fill in the following function to calculate and return the mean perplexities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8H8TJ9KzZMGT"},"outputs":[],"source":["def mean_perplexity(docs, authors):\n","    \"\"\"\n","    Evaluate the mean of the perplexity of the unknown Federalist papers for the language models\n","    from each of the three authors (again, using n=3 and alpha=1e-3)\n","\n","    Args:\n","        docs: list of strings, where each string represents a space-separated document\n","        authors: list of lists, which each list contains the author (or potential authors) of a given document\n","\n","    Returns: tuple: (perp_hamilton, perp_madison, perp_jay)\n","        perp_hamilton: floating point value, mean perplexity of the unknown Federalist papers for the language\n","                       models from Hamilton\n","        perp_madison: floating point value, mean perplexity of the unknown Federalist papers for the language\n","                       models from Madison\n","        perp_jay: floating point value, mean perplexity of the unknown Federalist papers for the language\n","                       models from Jay\n","    \"\"\"\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"r7ROaYKnZMGT"},"source":["Does the most likely author (i.e., the author with the smallest perplexity), match up with the author with the highest cosine similarity above?"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}